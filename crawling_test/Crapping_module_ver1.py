import time
import pandas as pd
import traceback
import random
import os
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup # ë°ì´í„° ì¶”ì¶œ ê°€ì†ìš©

# ë³‘ë ¬ ì²˜ë¦¬ ë° í”„ë¡œì„¸ìŠ¤ ê°„ ë™ê¸°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from multiprocessing import Pool, freeze_support, Manager

# --- ì„¤ì • ---
TARGET_RATINGS = ['ìµœê³ ', 'ì¢‹ìŒ', 'ë³´í†µ', 'ë³„ë¡œ', 'ë‚˜ì¨']
MAX_REVIEWS_PER_RATING = 100

def setup_driver(lock=None):
    """
    undetected_chromedriver ì´ˆê¸°í™” 
    (Headless + Eager Mode + ì´ë¯¸ì§€ ì°¨ë‹¨ ì ìš©)
    """
    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    
    # [í•µì‹¬ ë³€ê²½ 1] í˜ì´ì§€ ë¡œë”© ì „ëµ: Eager
    # DOM(HTML êµ¬ì¡°)ë§Œ ë¡œë“œë˜ë©´ ì´ë¯¸ì§€/CSS ë¡œë”©ì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì¦‰ì‹œ ì œì–´ê¶Œì„ ë„˜ê¹€
    options.page_load_strategy = 'eager'
    
    # [í•µì‹¬ ë³€ê²½ 2] ì´ë¯¸ì§€ ë¡œë”© ì•„ì˜ˆ ì°¨ë‹¨ (ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ë° ë Œë”ë§ ì‹œê°„ ì ˆì•½)
    prefs = {"profile.managed_default_content_settings.images": 2}
    options.add_experimental_option("prefs", prefs)

    # [ì†ë„ í–¥ìƒ] Headless ëª¨ë“œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    # options.add_argument("--headless=new") 
    
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    
    driver = None
    
    if lock: lock.acquire()
    
    try:
        driver = uc.Chrome(options=options, version_main=141)
    except Exception as e:
        try:
            driver = uc.Chrome(options=options)
        except Exception as e2:
            print(f"   [ì¹˜ëª…ì  ì˜¤ë¥˜] ë“œë¼ì´ë²„ ë¡œë“œ ì‹¤íŒ¨: {e2}")
    finally:
        if lock:
            time.sleep(1) 
            lock.release()
            
    return driver

def extract_reviews(driver, current_rating_filter):
    """
    BeautifulSoupì„ ì´ìš©í•œ ê³ ì† ë°ì´í„° ì¶”ì¶œ
    """
    reviews_data = []
    
    # ë¦¬ë·° ì•„ì´í…œì„ ì°¾ëŠ” XPath
    review_article_xpath = "//article[contains(@class, 'sdp-review__article__list') or contains(@class, 'twc-pt-[16px]')]"

    try:
        # Eager ëª¨ë“œì—¬ë„ ë°ì´í„°ê°€ ì‹¤ì œ DOMì— ê½‚í ë•Œê¹Œì§€ëŠ” ê¸°ë‹¤ë ¤ì•¼ í•¨
        WebDriverWait(driver, 5).until(
            EC.presence_of_all_elements_located((By.XPATH, review_article_xpath))
        )
    except TimeoutException:
        return []

    # Seleniumì€ HTML ì†ŒìŠ¤ë§Œ ë¤í”„í•˜ê³ , ë¶„ì„ì€ Python(BeautifulSoup)ì´ ìˆ˜í–‰
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    
    articles = soup.find_all('article', class_=lambda x: x and ('sdp-review__article__list' in x or 'twc-pt-[16px]' in x))
    
    for article in articles:
        try:
            def get_text(selector):
                el = article.select_one(selector)
                return el.get_text(strip=True) if el else ""

            author_el = article.select_one("span[data-member-id]")
            author = author_el.get_text(strip=True) if author_el else ""
            
            rating = len(article.select("i.twc-bg-full-star"))
            
            date = get_text("div.sdp-review__article__list__info__product-info__reg-date")
            if not date:
                try:
                    stars_div = article.select_one("div:has(> i.twc-bg-full-star)")
                    if stars_div:
                        date_div = stars_div.find_next_sibling("div")
                        if date_div: date = date_div.get_text(strip=True)
                except: pass
            
            product_option = get_text("div.sdp-review__article__list__info__product-info__name")
            if not product_option: 
                product_option = get_text("div.twc-my-\\[16px\\]")
            
            review_title = get_text("div.sdp-review__article__list__headline")
            if not review_title: 
                review_title = get_text("div.twc-mb-\\[8px\\].twc-font-bold")

            review_body = get_text("div.sdp-review__article__list__review__content")
            if not review_body: 
                review_body = get_text("div.twc-break-all")
            
            helpful = 0
            try: 
                help_div = article.select_one("div.sdp-review__article__list__help")
                if help_div and help_div.has_attr("data-count"):
                    helpful = int(help_div["data-count"])
                else:
                    help_text_div = article.find("div", string=lambda text: text and "ëª…ì—ê²Œ ë„ì›€ë˜ì—ˆìŠµë‹ˆë‹¤" in text)
                    if help_text_div:
                        text = help_text_div.get_text(strip=True)
                        helpful = int(text.split('ëª…')[0].replace(',', '').strip())
            except: 
                pass
            
            reviews_data.append({
                "ë³„ì í•„í„°": current_rating_filter,
                "ì‘ì„±ì": author, "í‰ì ": rating, "ë‚ ì§œ": date, "êµ¬ë§¤ì˜µì…˜": product_option,
                "ì œëª©": review_title, "ë‚´ìš©": review_body, "ë„ì›€ë¨": helpful
            })
        except: 
            continue
            
    return reviews_data

def apply_rating_filter(driver, wait, rating_name):
    """ë³„ì  í•„í„° ì ìš© (JS ê°•ì œ í´ë¦­)"""
    try:
        filter_btn = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div[role='combobox']")))
        
        if rating_name in filter_btn.text and "ëª¨ë“  ë³„ì " not in filter_btn.text:
            return True

        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", filter_btn)
        time.sleep(1) 
        driver.execute_script("arguments[0].click();", filter_btn)
        
        popup = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "[data-radix-popper-content-wrapper]")))
        option = popup.find_element(By.XPATH, f".//div[contains(text(), '{rating_name}')]")
        
        driver.execute_script("arguments[0].click();", option)
        
        wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, "[data-radix-popper-content-wrapper]")))
        time.sleep(1.5) 
        return True
    except Exception as e:
        print(f"   FAIL: [{rating_name}] í•„í„° ì§„ì… ì‹¤íŒ¨")
        return False

def scrape_single_rating(target_url, rating_name, lock):
    """ìµœì í™”ëœ ìˆ˜ì§‘ í•¨ìˆ˜ (Eager load + BeautifulSoup + Parallel)"""
    
    start_delay = random.uniform(0.5, 2.0)
    time.sleep(start_delay)
    
    driver = setup_driver(lock)
    if not driver: return []

    collected = []
    print(f"START: [{rating_name}] ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        wait = WebDriverWait(driver, 20)
        driver.get(target_url)
        
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//a[contains(text(),'ìƒí’ˆí‰')]")))
            ActionChains(driver).move_to_element(review_tab).click().perform()
        except TimeoutException:
            print(f"FAIL: [{rating_name}] ìƒí’ˆí‰ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []

        review_section = wait.until(EC.presence_of_element_located((By.ID, "sdpReview")))
        driver.execute_script("arguments[0].scrollIntoView(true);", review_section)
        
        if not apply_rating_filter(driver, wait, rating_name):
            print(f"FAIL: [{rating_name}] í•„í„° ì ìš© ì‹¤íŒ¨")
            return []

        visited_pages = set()
        consecutive_failures = 0

        while len(collected) < MAX_REVIEWS_PER_RATING:
            try:
                # í˜ì´ì§€ë„¤ì´ì…˜ ë¡œë”© ëŒ€ê¸°
                pagination_xpath = "//div[@data-page and @data-start and @data-end]"
                is_new_ui = False 
                pagination = None
                
                try:
                    pagination = WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.XPATH, pagination_xpath))
                    )
                    if "twc-mt-[24px]" in pagination.get_attribute("class"):
                        is_new_ui = True
                except TimeoutException:
                    pass 

                current_page = 1
                if pagination:
                    try:
                        if is_new_ui:
                            current_page = int(pagination.find_element(By.CSS_SELECTOR, "button[class*='twc-text-[#346aff]']").text.strip())
                        else:
                            current_page = int(pagination.find_element(By.CSS_SELECTOR, "button.selected").text.strip())
                    except: pass

                # --- ë¦¬ë·° ìˆ˜ì§‘ ---
                if current_page not in visited_pages:
                    new_reviews = extract_reviews(driver, rating_name)
                    if new_reviews:
                        collected.extend(new_reviews)
                        visited_pages.add(current_page)
                        consecutive_failures = 0
                        print(f"ING: [{rating_name}] {current_page}í˜ì´ì§€ {len(new_reviews)}ê°œ (ëˆ„ì : {len(collected)})")
                    else:
                        if pagination is None and current_page == 1:
                            print(f"INFO: [{rating_name}] ë¦¬ë·°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ -> ì¢…ë£Œ")
                            break 
                        consecutive_failures += 1
                
                if len(collected) >= MAX_REVIEWS_PER_RATING: break

                if pagination is None:
                    if len(collected) > 0:
                        print(f"INFO: [{rating_name}] ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ. ì¢…ë£Œ.")
                    break

                # --- ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ---
                if pagination:
                    next_btn = None
                    min_val = float('inf')

                    if is_new_ui:
                        buttons = pagination.find_elements(By.XPATH, ".//button[span]")
                    else:
                        buttons = pagination.find_elements(By.CSS_SELECTOR, "button.sdp-review__article__page__num")
                        
                    for btn in buttons:
                        try:
                            val = int(btn.text.strip())
                            if val not in visited_pages and val > current_page and val < min_val:
                                min_val = val
                                next_btn = btn
                        except: continue
                    
                    if next_btn:
                        # JS Click ì‚¬ìš© (Eager ëª¨ë“œì—ì„œ ë ˆì´ì•„ì›ƒ ì´ë™ ì‹œ ì•ˆì •ì„± í™•ë³´)
                        try: next_btn.click()
                        except: driver.execute_script("arguments[0].click();", next_btn)
                        
                        # [ìµœì í™”] í˜ì´ì§€ ì´ë™ ëŒ€ê¸° ì‹œê°„ì„ ì¡°ê¸ˆ ë” ì¤„ì„ (ì´ë¯¸ì§€ ë¡œë”© ì•ˆ í•˜ë¯€ë¡œ)
                        time.sleep(random.uniform(1.0, 1.5)) 
                    else:
                        try:
                            next_group = pagination.find_element(By.XPATH, ".//button[.//svg[not(contains(@class, 'twc-rotate'))]]")
                            if next_group.is_enabled():
                                try: next_group.click()
                                except: driver.execute_script("arguments[0].click();", next_group)
                                time.sleep(random.uniform(1.5, 2.0))
                            else:
                                print(f"INFO: [{rating_name}] ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ (ì´ {len(collected)}ê°œ). ì¢…ë£Œ.")
                                break
                        except:
                            print(f"INFO: [{rating_name}] ë” ì´ìƒ ì´ë™í•  í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤ (ì´ {len(collected)}ê°œ). ì¢…ë£Œ.")
                            break
                else:
                    if consecutive_failures >= 3: break
                    time.sleep(1)

            except Exception:
                consecutive_failures += 1
                if consecutive_failures >= 5: break
                time.sleep(1)

    except Exception as e:
        print(f"ERROR: [{rating_name}] ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        if driver:
            try: driver.quit()
            except: pass
    
    return collected[:MAX_REVIEWS_PER_RATING]

def scrape_wrapper(args):
    return scrape_single_rating(*args)

if __name__ == "__main__":
    freeze_support()

    target_url = "https://www.coupang.com/vp/products/7666070794?itemId=26528256734&searchId=feed-916be5672b844ae3a868a9ae4de0a60d-view_together_ads-P7224339339&vendorItemId=93409074156&sourceType=SDP_ADS&clickEventId=42651fd0-cb6e-11f0-bf3a-f1516b466eb7"
    
    print("=== ë³‘ë ¬ ë¦¬ë·° ìŠ¤í¬ë˜í•‘ ì‹œì‘ (Eager Mode + Image Block + BS4) ===")
    
    m = Manager()
    lock = m.Lock()

    tasks = [(target_url, rating, lock) for rating in TARGET_RATINGS]

    start_time = time.time()
    all_results = []

    with Pool(processes=len(TARGET_RATINGS)) as pool:
        results_list = pool.map(scrape_wrapper, tasks)
        for result in results_list: 
            all_results.extend(result)

    end_time = time.time()
    print(f"\n=== ì „ì²´ ìˆ˜ì§‘ ì¢…ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ===")

    if all_results:
        df = pd.DataFrame(all_results)
        file_name = "coupang_reviews_final_speedup.xlsx"
        df.to_excel(file_name, index=False)
        print(f"\nğŸ‰ [ì „ì²´ ì™„ë£Œ] ì´ {len(all_results)}ê°œì˜ ë¦¬ë·°ê°€ '{file_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\n[ì•Œë¦¼] ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")