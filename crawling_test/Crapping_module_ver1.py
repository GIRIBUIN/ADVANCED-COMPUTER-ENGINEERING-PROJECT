import time
import pandas as pd
import traceback
import random
import os
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.common.action_chains import ActionChains

# ë³‘ë ¬ ì²˜ë¦¬ ë° í”„ë¡œì„¸ìŠ¤ ê°„ ë™ê¸°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from multiprocessing import Pool, freeze_support, Manager

# --- ì„¤ì • ---
TARGET_RATINGS = ['ìµœê³ ', 'ì¢‹ìŒ', 'ë³´í†µ', 'ë³„ë¡œ', 'ë‚˜ì¨']
MAX_REVIEWS_PER_RATING = 100

def setup_driver(lock=None):
    """
    undetected_chromedriver ì´ˆê¸°í™” (Headless ì ìš© & ì¶©ëŒ ë°©ì§€)
    """
    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    
    # [ì†ë„ í–¥ìƒ] Headless ëª¨ë“œ ì ìš© (íƒì§€ ìš°íšŒë¥¼ ìœ„í•´ 'new' ì˜µì…˜ ì‚¬ìš©)
    # ë§Œì•½ ì‹¤í–‰ ì‹œ ì°¨ë‹¨ë˜ê±°ë‚˜ ë¦¬ë·°ê°€ 0ê°œë¼ë©´ ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
    #options.add_argument("--headless=new") 
    
    # ë¦¬ì†ŒìŠ¤ ì ˆì•½ ì˜µì…˜
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    
    driver = None
    
    # ë“œë¼ì´ë²„ íŒŒì¼ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë½ íšë“
    if lock: lock.acquire()
    
    try:
        # íŠ¹ì • ë²„ì „(141) ì§€ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶¤)
        driver = uc.Chrome(options=options, version_main=141)
    except Exception as e:
        try:
            # ì‹¤íŒ¨ ì‹œ ìë™ ê°ì§€ ëª¨ë“œë¡œ ì¬ì‹œë„
            driver = uc.Chrome(options=options)
        except Exception as e2:
            print(f"   [ì¹˜ëª…ì  ì˜¤ë¥˜] ë“œë¼ì´ë²„ ë¡œë“œ ì‹¤íŒ¨: {e2}")
    finally:
        # ë“œë¼ì´ë²„ ë¡œë“œ í›„ ë½ í•´ì œ (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ì§„ì… í—ˆìš©)
        if lock:
            time.sleep(1) 
            lock.release()
            
    return driver

def extract_reviews(driver, current_rating_filter):
    """ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ (êµ¬í˜• UI / ì‹ í˜• UI í˜¸í™˜)"""
    reviews_data = []
    
    # ë¦¬ë·° ì•„ì´í…œì„ ì°¾ëŠ” í¬ê´„ì ì¸ XPath
    review_article_xpath = "//article[contains(@class, 'sdp-review__article__list') or contains(@class, 'twc-pt-[16px]')]"

    try:
        # ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì§§ê²Œ ëŒ€ê¸°
        WebDriverWait(driver, 5).until(
            EC.presence_of_all_elements_located((By.XPATH, review_article_xpath))
        )
    except TimeoutException:
        return []

    articles = driver.find_elements(By.XPATH, review_article_xpath)
    
    for article in articles:
        try:
            def get_text(selector):
                try: return article.find_element(By.CSS_SELECTOR, selector).text.strip()
                except: return ""

            # ì‘ì„±ì
            author = article.find_element(By.CSS_SELECTOR, "span[data-member-id]").text.strip()
            
            # í‰ì 
            rating = len(article.find_elements(By.CSS_SELECTOR, "i.twc-bg-full-star"))
            
            # ë‚ ì§œ
            date = get_text("div.sdp-review__article__list__info__product-info__reg-date")
            if not date: 
                date = article.find_element(By.XPATH, ".//div[i[contains(@class, 'twc-bg-full-star')]]/following-sibling::div").text.strip()
            
            # êµ¬ë§¤ì˜µì…˜
            product_option = get_text("div.sdp-review__article__list__info__product-info__name")
            if not product_option: 
                product_option = get_text("div.twc-my-\\[16px\\]")
            
            # ë¦¬ë·° ì œëª©
            review_title = get_text("div.sdp-review__article__list__headline")
            if not review_title: 
                review_title = get_text("div.twc-mb-\\[8px\\].twc-font-bold")

            # ë¦¬ë·° ë‚´ìš©
            review_body = get_text("div.sdp-review__article__list__review__content")
            if not review_body: 
                review_body = get_text("div.twc-break-all")
            
            # ë„ì›€ë¨ ì¹´ìš´íŠ¸
            helpful = 0
            try: 
                helpful = int(article.find_element(By.CSS_SELECTOR, "div.sdp-review__article__list__help").get_attribute("data-count"))
            except: 
                try:
                    helpful_text = article.find_element(By.XPATH, ".//div[contains(text(), 'ëª…ì—ê²Œ ë„ì›€ë˜ì—ˆìŠµë‹ˆë‹¤.')]").text
                    helpful = int(helpful_text.split('ëª…')[0].replace(',', '').strip())
                except:
                    pass
            
            reviews_data.append({
                "ë³„ì í•„í„°": current_rating_filter,
                "ì‘ì„±ì": author, "í‰ì ": rating, "ë‚ ì§œ": date, "êµ¬ë§¤ì˜µì…˜": product_option,
                "ì œëª©": review_title, "ë‚´ìš©": review_body, "ë„ì›€ë¨": helpful
            })
        except: 
            continue
    return reviews_data

def apply_rating_filter(driver, wait, rating_name):
    """ë³„ì  í•„í„° ì ìš©"""
    try:
        filter_btn = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "div[role='combobox']")))
        
        if rating_name in filter_btn.text and "ëª¨ë“  ë³„ì " not in filter_btn.text:
            return True

        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", filter_btn)
        time.sleep(0.5)
        filter_btn.click()
        
        popup = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "[data-radix-popper-content-wrapper]")))
        option = popup.find_element(By.XPATH, f".//div[contains(text(), '{rating_name}')]")
        option.click()
        
        wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, "[data-radix-popper-content-wrapper]")))
        time.sleep(1.5) # í•„í„° ì ìš© í›„ ë¡œë”© ëŒ€ê¸°
        return True
    except Exception as e:
        # print(f"[{rating_name}] í•„í„° ì ìš© ì‹¤íŒ¨: {str(e)[:50]}")
        return False

def scrape_single_rating(target_url, rating_name, lock):
    """ìŠ¤ë§ˆíŠ¸ ëŒ€ê¸°(Dynamic Wait)ë¥¼ ì ìš©í•˜ì—¬ ì†ë„ë¥¼ ìµœì í™”í•œ ìˆ˜ì§‘ í•¨ìˆ˜"""
    
    # ì´ˆê¸° ì§„ì… ì‹œ í”„ë¡œì„¸ìŠ¤ ëª°ë¦¼ ë°©ì§€ (0.5~2ì´ˆ ëœë¤ ëŒ€ê¸°)
    start_delay = random.uniform(0.5, 2.0)
    time.sleep(start_delay)
    
    driver = setup_driver(lock)
    if not driver: return []

    collected = []
    print(f"START: [{rating_name}] (Headless) ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        # [ì†ë„ ìµœì í™”] ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì„¤ì •
        wait = WebDriverWait(driver, 20)
        driver.get(target_url)
        
        # ìƒí’ˆí‰ íƒ­ í´ë¦­
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//a[contains(text(),'ìƒí’ˆí‰')]")))
            ActionChains(driver).move_to_element(review_tab).click().perform()
        except TimeoutException:
            print(f"FAIL: [{rating_name}] ìƒí’ˆí‰ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []

        # ë¦¬ë·° ì„¹ì…˜ ë¡œë”©
        review_section = wait.until(EC.presence_of_element_located((By.ID, "sdpReview")))
        driver.execute_script("arguments[0].scrollIntoView(true);", review_section)
        
        # ë³„ì  í•„í„° ì ìš©
        if not apply_rating_filter(driver, wait, rating_name):
            print(f"FAIL: [{rating_name}] í•„í„° ì ìš© ì‹¤íŒ¨")
            return []

        visited_pages = set()
        consecutive_failures = 0

        while len(collected) < MAX_REVIEWS_PER_RATING:
            try:
                # í˜ì´ì§€ë„¤ì´ì…˜ ë°” ê°ì§€ (ìµœëŒ€ 5ì´ˆë§Œ ëŒ€ê¸°)
                pagination_xpath = "//div[@data-page and @data-start and @data-end]"
                is_new_ui = False 
                pagination = None
                
                try:
                    pagination = WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.XPATH, pagination_xpath))
                    )
                    if "twc-mt-[24px]" in pagination.get_attribute("class"):
                        is_new_ui = True
                except TimeoutException:
                    pass # ì—†ìœ¼ë©´ ë‹¨ì¼ í˜ì´ì§€ì¼ ìˆ˜ ìˆìŒ

                # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ íŒŒì•…
                current_page = 1
                if pagination:
                    try:
                        if is_new_ui:
                            current_page = int(pagination.find_element(By.CSS_SELECTOR, "button[class*='twc-text-[#346aff]']").text.strip())
                        else:
                            current_page = int(pagination.find_element(By.CSS_SELECTOR, "button.selected").text.strip())
                    except: pass

                # --- ë¦¬ë·° ìˆ˜ì§‘ ---
                if current_page not in visited_pages:
                    new_reviews = extract_reviews(driver, rating_name)
                    if new_reviews:
                        collected.extend(new_reviews)
                        visited_pages.add(current_page)
                        consecutive_failures = 0
                        print(f"ING: [{rating_name}] {current_page}í˜ì´ì§€ {len(new_reviews)}ê°œ (ëˆ„ì : {len(collected)})")
                    else:
                        if pagination is None and current_page == 1:
                            print(f"INFO: [{rating_name}] ë¦¬ë·° ì—†ìŒ -> ì¢…ë£Œ")
                            break 
                        consecutive_failures += 1
                
                if len(collected) >= MAX_REVIEWS_PER_RATING: break

                # --- ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ---
                if pagination:
                    next_btn = None
                    min_val = float('inf')

                    if is_new_ui:
                        buttons = pagination.find_elements(By.XPATH, ".//button[span]")
                    else:
                        buttons = pagination.find_elements(By.CSS_SELECTOR, "button.sdp-review__article__page__num")
                        
                    for btn in buttons:
                        try:
                            val = int(btn.text.strip())
                            if val not in visited_pages and val > current_page and val < min_val:
                                min_val = val
                                next_btn = btn
                        except: continue
                    
                    if next_btn:
                        try: next_btn.click()
                        except: driver.execute_script("arguments[0].click();", next_btn)
                        
                        # [ì†ë„ ìµœì í™”] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë´‡ íƒì§€ ë°©ì§€ìš© ìµœì†Œ ë”œë ˆì´ í¬í•¨)
                        time.sleep(random.uniform(1.5, 2.5)) 
                        
                    else:
                        # ë‹¤ìŒ ê·¸ë£¹(>) ë²„íŠ¼ ì²˜ë¦¬
                        try:
                            next_group = pagination.find_element(By.XPATH, ".//button[.//svg[not(contains(@class, 'twc-rotate'))]]")
                            if next_group.is_enabled():
                                try: next_group.click()
                                except: driver.execute_script("arguments[0].click();", next_group)
                                time.sleep(random.uniform(2.0, 3.0))
                            else:
                                break
                        except: break
                else:
                    if consecutive_failures >= 3: break
                    time.sleep(2)

            except Exception:
                consecutive_failures += 1
                if consecutive_failures >= 5: break
                time.sleep(1)

    except Exception as e:
        print(f"ERROR: [{rating_name}] ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        if driver:
            try: driver.quit()
            except: pass
    
    return collected[:MAX_REVIEWS_PER_RATING]

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
def scrape_wrapper(args):
    return scrape_single_rating(*args)

if __name__ == "__main__":
    # Windows ë©€í‹°í”„ë¡œì„¸ì‹± í•„ìˆ˜ ì„¤ì •
    freeze_support()

    # ëŒ€ìƒ URL (ì—¬ê¸°ì— ì›í•˜ì‹œëŠ” ìƒí’ˆ URLì„ ì…ë ¥í•˜ì„¸ìš”)
    target_url = "https://www.coupang.com/vp/products/7224339339?vendorItemId=3051369121&sourceType=SDP_ALSO_VIEWED"
    
    print("=== ë³‘ë ¬ ë¦¬ë·° ìŠ¤í¬ë˜í•‘ ì‹œì‘ (í”„ë¡œì„¸ìŠ¤ 5ê°œ / Headless) ===")
    
    # í”„ë¡œì„¸ìŠ¤ ê°„ ê³µìœ  ë½ ìƒì„±
    m = Manager()
    lock = m.Lock()

    # ì‘ì—… ëª©ë¡ ìƒì„±
    tasks = [(target_url, rating, lock) for rating in TARGET_RATINGS]

    start_time = time.time()
    all_results = []

    # í”„ë¡œì„¸ìŠ¤ í’€ ê°€ë™ (5ê°œ ë™ì‹œ ì‹¤í–‰)
    with Pool(processes=len(TARGET_RATINGS)) as pool:
        results_list = pool.map(scrape_wrapper, tasks)
        for result in results_list:
            all_results.extend(result)

    end_time = time.time()
    print(f"\n=== ì „ì²´ ìˆ˜ì§‘ ì¢…ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ===")

    if all_results:
        df = pd.DataFrame(all_results)
        file_name = "coupang_reviews_final_parallel.xlsx"
        df.to_excel(file_name, index=False)
        print(f"\nğŸ‰ [ì „ì²´ ì™„ë£Œ] ì´ {len(all_results)}ê°œì˜ ë¦¬ë·°ê°€ '{file_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\n[ì•Œë¦¼] ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤. (Headless íƒì§€ ì—¬ë¶€ í™•ì¸ í•„ìš”)")